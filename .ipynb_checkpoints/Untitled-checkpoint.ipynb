{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff94233-e9f6-4c5e-b873-58808b1d4e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|##########| 2/2 [00:05<00:00,  2.65s/it]\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from threading import Thread\n",
    "# Load environment variables and Hugging Face token\n",
    "load_dotenv()\n",
    "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "\n",
    "text_gen_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token=huggingface_token)\n",
    "text_gen_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token=huggingface_token)\n",
    "\n",
    "\n",
    "device_text = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "text_gen_model = text_gen_model.to(device_text)\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc8ece-4416-4be7-a4fa-b709c758c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text generation...\n",
      "Starting text generation inside...\n",
      "tensor([[     2,  26941,  65222,    590, 117223,  47912,  56972,  34091,    586,\n",
      "          38516]], device='mps:0')\n",
      "iam inside\n",
      "output_sequence tensor([[     2,  26941,  65222,    590, 117223,  47912,  56972,  34091,    586,\n",
      "          38516, 235336,    109,    688, 235274, 235265,  97261,    861,  65002,\n",
      "         116742,  11109,    476,   4318,    577,  13849,    861,   2811,   1160,\n",
      "            578,    573,   8385,  17230,    576,    573,   3542, 235265,    109,\n",
      "            688, 235284, 235265,  73763,    611,    573,   2185, 116742,  11109,\n",
      "           1009,   1069,    577,  10398,    611]], device='mps:0')\n",
      "Generated text: HOW SHOULD I FEEL AFTER COMPLETING A PROJECT?\n",
      "\n",
      "**1. Acknowledge your accomplishment.** Take a moment to appreciate your hard work and the successful completion of the project.\n",
      "\n",
      "**2. Reflect on the process.** Take some time to reflect on\n"
     ]
    }
   ],
   "source": [
    "# Global variable to control the recording state\n",
    "is_recording = False\n",
    "audio_data = np.array([])\n",
    "\n",
    "\n",
    "def record_audio(fs=16000):\n",
    "    \"\"\"Continuously record audio until stopped.\"\"\"\n",
    "    global is_recording, audio_data\n",
    "    with sd.InputStream(samplerate=fs, channels=1, callback=callback):\n",
    "        while is_recording:\n",
    "            sd.sleep(100)  # Small sleep to avoid locking up the CPU\n",
    "\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    \"\"\"This is called for each audio block from the microphone.\"\"\"\n",
    "    global audio_data\n",
    "    audio_data = np.append(audio_data, indata.copy())\n",
    "\n",
    "\n",
    "def toggle_recording():\n",
    "    \"\"\"Toggle the recording state between start and stop.\"\"\"\n",
    "    global is_recording, audio_data\n",
    "    if not is_recording:\n",
    "        # Start recording\n",
    "        is_recording = True\n",
    "        audio_data = np.array([])\n",
    "        Thread(target=record_audio).start()  # Start recording in a background thread\n",
    "    else:\n",
    "        # Stop recording\n",
    "        is_recording = False\n",
    "        process_audio(audio_data)  # Process the recorded audio\n",
    "\n",
    "\n",
    "def process_audio(audio_data, fs=16000):\n",
    "    \"\"\"Process the recorded audio and update the transcription label.\"\"\"\n",
    "    # Preprocess the audio to match the model's expected format\n",
    "    input_values = processor(\n",
    "        audio_data, return_tensors=\"pt\", sampling_rate=fs\n",
    "    ).input_values\n",
    "\n",
    "    # Move to the same device as the model\n",
    "    input_values = input_values.to(model.device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # Decode the predicted ids\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    # Update the transcription label text in the main thread\n",
    "    def update_transcription_label():\n",
    "        transcription_label.config(text=f\"Transcribed Text: {transcription}\")\n",
    "        generate_text(transcription)\n",
    "\n",
    "    root.after(0, update_transcription_label)\n",
    "\n",
    "\n",
    "def generate_text(transcription):\n",
    "    print(\"Starting text generation...\")\n",
    "\n",
    "    def background_generate():\n",
    "        print(\"Starting text generation inside...\")\n",
    "        input_ids = text_gen_tokenizer.encode(transcription, return_tensors=\"pt\").to(device_text)\n",
    "        print(input_ids)\n",
    "        try:\n",
    "            # Generate text using the model and tokenizer\n",
    "            print(\"iam inside\")\n",
    "            output_sequences = text_gen_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "            print(\"output_sequence\", output_sequences)\n",
    "            generated_text = text_gen_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            print(\"Generated text:\", generated_text)  # For debugging\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            generated_text = \"Error in generating text.\"\n",
    "\n",
    "        # Function to update the GUI with the generated text\n",
    "        def update_generated_text_label():\n",
    "            generated_text_label.config(text=f\"Model Response: {generated_text}\")\n",
    "\n",
    "        # Schedule the GUI update to run in the main thread\n",
    "        root.after(0, update_generated_text_label)\n",
    "\n",
    "    # Run the text generation in a background thread to avoid blocking the GUI\n",
    "    Thread(target=background_generate).start()\n",
    "\n",
    "\n",
    "# Set up the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Voice Recorder\")\n",
    "\n",
    "# Add a record button\n",
    "record_button = tk.Button(root, text=\"Start Recording\", command=toggle_recording)\n",
    "record_button.pack(pady=20)\n",
    "\n",
    "# Add a label widget for displaying the transcription\n",
    "transcription_label = tk.Label(\n",
    "    root, text=\"Transcription will appear here...\", wraplength=400\n",
    ")\n",
    "transcription_label.pack(pady=10)\n",
    "\n",
    "generated_text_label = tk.Label(\n",
    "    root, text=\"Generated text will appear here...\", wraplength=2000\n",
    ")\n",
    "generated_text_label.pack(pady=10)\n",
    "\n",
    "\n",
    "def update_button_text():\n",
    "    \"\"\"Update the button text based on the recording state.\"\"\"\n",
    "    if is_recording:\n",
    "        record_button.config(text=\"Stop Recording\")\n",
    "    else:\n",
    "        record_button.config(text=\"Start Recording\")\n",
    "    root.after(100, update_button_text)\n",
    "\n",
    "\n",
    "root.after(100, update_button_text)  # Check every 100ms to update the button text\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf09c73-a7a0-49df-b85b-6f8435f90662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
