{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ff94233-e9f6-4c5e-b873-58808b1d4e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|##########| 2/2 [00:05<00:00,  2.60s/it]\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from threading import Thread\n",
    "# Load environment variables and Hugging Face token\n",
    "load_dotenv()\n",
    "huggingface_token = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "\n",
    "text_gen_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token=huggingface_token)\n",
    "text_gen_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token=huggingface_token)\n",
    "\n",
    "\n",
    "device_text = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "text_gen_model = text_gen_model.to(device_text)\n",
    "\n",
    "# Initialize the processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc8ece-4416-4be7-a4fa-b709c758c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text generation...\n",
      "Starting text generation inside...\n",
      "tensor([[     2,  33574,  14975,    586,  50471,  30825,   9661,  41533, 128362,\n",
      "           2148,   5167,  18744,  26224, 111965,  92364,  54094,    595,  11047,\n",
      "           1905]], device='mps:0')\n",
      "iam inside\n",
      "output_sequence tensor([[     2,  33574,  14975,    586,  50471,  30825,   9661,  41533, 128362,\n",
      "           2148,   5167,  18744,  26224, 111965,  92364,  54094,    595,  11047,\n",
      "           1905,   3228, 179743, 235265,    109, 235285,   1144,    476,   2910,\n",
      "           5255,   2091, 235269,  17363,    731,   6238, 235265,    590,   1144,\n",
      "          13920,    576,  31928,   3515, 235290,  14683,   2793,    575,   3590,\n",
      "            577,    476,   5396,   3001,    576,  73815,    578,   3920, 235265,\n",
      "            109,    688,   4858,    708,   1009,   8944,    576,    970,  22466,\n",
      "          66058,    109, 235287,    590,    798,   5598,   8965,    578,  29632,\n",
      "         235265,    108, 235287,    590,    798,  19631,  17044, 235265,    108,\n",
      "         235287,    590,    798,  62846,   2793, 235265,    108, 235287,    590,\n",
      "            798,   5598,   2167,  13049,    576,  12354,   3381, 235269,   1582,\n",
      "            685,   4296,    578,   3409, 235265,    108, 235287,    590,    798,\n",
      "           3448,   3920,    611,    476,   5396,   3001,    576,  16254, 235265,\n",
      "            109,    688, 235285,   1144,   2076,   1362,   3505, 235269,    901,\n",
      "            590,   1144,   6044,    888,   2652,   1535,   1744, 116742,    109,\n",
      "            688,   2299,    798,    590,   1707,    692,   3646, 235336,    688,\n",
      "              1]], device='mps:0')\n",
      "Generated text: YOU ARE A VERY GOOD YOU KNOW SORT OF AM GENATING TESMONT EXT MORNEL AND EVENING.\n",
      "\n",
      "I am a large language model, trained by Google. I am capable of generating human-quality text in response to a wide range of prompts and questions.\n",
      "\n",
      "**Here are some examples of my capabilities:**\n",
      "\n",
      "* I can write stories and poems.\n",
      "* I can translate languages.\n",
      "* I can summarize text.\n",
      "* I can write different kinds of creative content, such as music and code.\n",
      "* I can answer questions on a wide range of topics.\n",
      "\n",
      "**I am still under development, but I am learning new things every day.**\n",
      "\n",
      "**How can I help you today?**\n"
     ]
    }
   ],
   "source": [
    "# Global variable to control the recording state\n",
    "is_recording = False\n",
    "audio_data = np.array([])\n",
    "\n",
    "\n",
    "def record_audio(fs=16000):\n",
    "    \"\"\"Continuously record audio until stopped.\"\"\"\n",
    "    global is_recording, audio_data\n",
    "    with sd.InputStream(samplerate=fs, channels=1, callback=callback):\n",
    "        while is_recording:\n",
    "            sd.sleep(100)  # Small sleep to avoid locking up the CPU\n",
    "\n",
    "\n",
    "def callback(indata, frames, time, status):\n",
    "    \"\"\"This is called for each audio block from the microphone.\"\"\"\n",
    "    global audio_data\n",
    "    audio_data = np.append(audio_data, indata.copy())\n",
    "\n",
    "\n",
    "def toggle_recording():\n",
    "    \"\"\"Toggle the recording state between start and stop.\"\"\"\n",
    "    global is_recording, audio_data\n",
    "    if not is_recording:\n",
    "        # Start recording\n",
    "        is_recording = True\n",
    "        audio_data = np.array([])\n",
    "        Thread(target=record_audio).start()  # Start recording in a background thread\n",
    "    else:\n",
    "        # Stop recording\n",
    "        is_recording = False\n",
    "        process_audio(audio_data)  # Process the recorded audio\n",
    "\n",
    "\n",
    "def process_audio(audio_data, fs=16000):\n",
    "    \"\"\"Process the recorded audio and update the transcription label.\"\"\"\n",
    "    # Preprocess the audio to match the model's expected format\n",
    "    input_values = processor(\n",
    "        audio_data, return_tensors=\"pt\", sampling_rate=fs\n",
    "    ).input_values\n",
    "\n",
    "    # Move to the same device as the model\n",
    "    input_values = input_values.to(model.device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # Decode the predicted ids\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    # Update the transcription label text in the main thread\n",
    "    def update_transcription_label():\n",
    "        transcription_label.config(text=f\"Transcribed Text: {transcription}\")\n",
    "        generate_text(transcription)\n",
    "\n",
    "    root.after(0, update_transcription_label)\n",
    "\n",
    "\n",
    "def generate_text(transcription):\n",
    "    print(\"Starting text generation...\")\n",
    "\n",
    "    def background_generate():\n",
    "        print(\"Starting text generation inside...\")\n",
    "        input_ids = text_gen_tokenizer.encode(transcription, return_tensors=\"pt\").to(device_text)\n",
    "        print(input_ids)\n",
    "        try:\n",
    "            # Generate text using the model and tokenizer\n",
    "            print(\"iam inside\")\n",
    "            output_sequences = text_gen_model.generate(input_ids, max_length=2000, num_return_sequences=1)\n",
    "            print(\"output_sequence\", output_sequences)\n",
    "            generated_text = text_gen_tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            print(\"Generated text:\", generated_text)  # For debugging\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "            generated_text = \"Error in generating text.\"\n",
    "\n",
    "        # Function to update the GUI with the generated text\n",
    "        def update_generated_text_label():\n",
    "            generated_text_label.config(text=f\"Model Response: {generated_text}\")\n",
    "\n",
    "        # Schedule the GUI update to run in the main thread\n",
    "        root.after(0, update_generated_text_label)\n",
    "\n",
    "    # Run the text generation in a background thread to avoid blocking the GUI\n",
    "    Thread(target=background_generate).start()\n",
    "\n",
    "\n",
    "# Set up the GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Voice Recorder\")\n",
    "\n",
    "# Add a record button\n",
    "record_button = tk.Button(root, text=\"Start Recording\", command=toggle_recording)\n",
    "record_button.pack(pady=20)\n",
    "\n",
    "# Add a label widget for displaying the transcription\n",
    "transcription_label = tk.Label(\n",
    "    root, text=\"Transcription will appear here...\", wraplength=400\n",
    ")\n",
    "transcription_label.pack(pady=10)\n",
    "\n",
    "generated_text_label = tk.Label(\n",
    "    root, text=\"Generated text will appear here...\", wraplength=2000\n",
    ")\n",
    "generated_text_label.pack(pady=10)\n",
    "\n",
    "\n",
    "def update_button_text():\n",
    "    \"\"\"Update the button text based on the recording state.\"\"\"\n",
    "    if is_recording:\n",
    "        record_button.config(text=\"Stop Recording\")\n",
    "    else:\n",
    "        record_button.config(text=\"Start Recording\")\n",
    "    root.after(100, update_button_text)\n",
    "\n",
    "\n",
    "root.after(100, update_button_text)  # Check every 100ms to update the button text\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf09c73-a7a0-49df-b85b-6f8435f90662",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
